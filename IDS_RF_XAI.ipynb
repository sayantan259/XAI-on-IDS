{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning attribute name to dataset\n",
    "dataset_train = pd.read_csv(\"KDDTrain+_2.csv\", header=None, names = col_names)\n",
    "dataset_test = pd.read_csv(\"KDDTest+_2.csv\", header=None, names = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n",
    "# explore categorical features\n",
    "print('Training set:')\n",
    "for col_name in dataset_train.columns:\n",
    "    if dataset_train[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(dataset_train[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "#see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\n",
    "print()\n",
    "print('Distribution of categories in service:')\n",
    "print(dataset_train['service'].value_counts().sort_values(ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "print('Test set:')\n",
    "for col_name in dataset_test.columns:\n",
    "    if dataset_test[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(dataset_test[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=['protocol_type', 'service', 'flag']\n",
    "# insert code to get a list of categorical columns into a variable, categorical_columns\n",
    "categorical_columns=['protocol_type', 'service', 'flag'] \n",
    " # Get the categorical values into a 2D numpy array\n",
    "dataset_train_categorical_values = dataset_train[categorical_columns]\n",
    "dataset_test_categorical_values = dataset_test[categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protocol type\n",
    "unique_protocol=sorted(dataset_train.protocol_type.unique())\n",
    "string1 = 'Protocol_type_'\n",
    "unique_protocol2=[string1 + x for x in unique_protocol]\n",
    "# service\n",
    "unique_service=sorted(dataset_train.service.unique())\n",
    "string2 = 'service_'\n",
    "unique_service2=[string2 + x for x in unique_service]\n",
    "# flag\n",
    "unique_flag=sorted(dataset_train.flag.unique())\n",
    "string3 = 'flag_'\n",
    "unique_flag2=[string3 + x for x in unique_flag]\n",
    "# put together\n",
    "dumcols=unique_protocol2 + unique_service2 + unique_flag2\n",
    "print(dumcols)\n",
    "\n",
    "#do same for test set\n",
    "unique_service_test=sorted(dataset_test.service.unique())\n",
    "unique_service2_test=[string2 + x for x in unique_service_test]\n",
    "testdumcols=unique_protocol2 + unique_service2_test + unique_flag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform categorical features into numbers using LabelEncoder()\n",
    "dataset_train_categorical_values_enc=dataset_train_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "print(dataset_train_categorical_values_enc.head())\n",
    "# test set\n",
    "dataset_test_categorical_values_enc=dataset_test_categorical_values.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot-EncodingÂ¶\n",
    "enc = OneHotEncoder()\n",
    "dataset_train_categorical_values_encenc = enc.fit_transform(dataset_train_categorical_values_enc)\n",
    "dataset_train_cat_data = pd.DataFrame(dataset_train_categorical_values_encenc.toarray(),columns=dumcols)\n",
    "# test set\n",
    "dataset_test_categorical_values_encenc = enc.fit_transform(dataset_test_categorical_values_enc)\n",
    "dataset_test_cat_data = pd.DataFrame(dataset_test_categorical_values_encenc.toarray(),columns=testdumcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainservice=dataset_train['service'].tolist()\n",
    "testservice= dataset_test['service'].tolist()\n",
    "difference=list(set(trainservice) - set(testservice))\n",
    "string = 'service_'\n",
    "difference=[string + x for x in difference]\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in difference:\n",
    "    dataset_test_cat_data[col] = 0\n",
    "dataset_test_cat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join encoded categorical dataframe with the non-categorical dataframe\n",
    "newdf=dataset_train.join(dataset_train_cat_data)\n",
    "newdf.drop('flag', axis=1, inplace=True)\n",
    "newdf.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf.drop('service', axis=1, inplace=True)\n",
    "# test data\n",
    "newdf_test=dataset_test.join(dataset_test_cat_data)\n",
    "newdf_test.drop('flag', axis=1, inplace=True)\n",
    "newdf_test.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf_test.drop('service', axis=1, inplace=True)\n",
    "print(newdf.shape)\n",
    "print(newdf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take label column\n",
    "labeldf=newdf['label']\n",
    "labeldf_test=newdf_test['label']\n",
    "# change the label column\n",
    "newlabeldf=labeldf.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "newlabeldf_test=labeldf_test.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "# put the new label column back\n",
    "newdf['label'] = newlabeldf\n",
    "newdf_test['label'] = newlabeldf_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_DoS = [2,3,4]\n",
    "to_drop_Probe = [1,3,4]\n",
    "to_drop_R2L = [1,2,4]\n",
    "to_drop_U2R = [1,2,3]\n",
    "DoS_df=newdf[~newdf['label'].isin(to_drop_DoS)];\n",
    "Probe_df=newdf[~newdf['label'].isin(to_drop_Probe)];\n",
    "R2L_df=newdf[~newdf['label'].isin(to_drop_R2L)];\n",
    "U2R_df=newdf[~newdf['label'].isin(to_drop_U2R)];\n",
    "\n",
    "#test\n",
    "DoS_df_test=newdf_test[~newdf_test['label'].isin(to_drop_DoS)];\n",
    "Probe_df_test=newdf_test[~newdf_test['label'].isin(to_drop_Probe)];\n",
    "R2L_df_test=newdf_test[~newdf_test['label'].isin(to_drop_R2L)];\n",
    "U2R_df_test=newdf_test[~newdf_test['label'].isin(to_drop_U2R)];\n",
    "print('Train:')\n",
    "print('Dimensions of DoS:' ,DoS_df.shape)\n",
    "print('Dimensions of Probe:' ,Probe_df.shape)\n",
    "print('Dimensions of R2L:' ,R2L_df.shape)\n",
    "print('Dimensions of U2R:' ,U2R_df.shape)\n",
    "print('Test:')\n",
    "print('Dimensions of DoS:' ,DoS_df_test.shape)\n",
    "print('Dimensions of Probe:' ,Probe_df_test.shape)\n",
    "print('Dimensions of R2L:' ,R2L_df_test.shape)\n",
    "print('Dimensions of U2R:' ,U2R_df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframes into X & Y\n",
    "# assign X as a dataframe of feautures and Y as a series of outcome variables\n",
    "X_DoS = DoS_df.drop(columns='label')\n",
    "Y_DoS = DoS_df['label']\n",
    "X_Probe = Probe_df.drop(columns='label')\n",
    "Y_Probe = Probe_df['label']\n",
    "X_R2L = R2L_df.drop(columns='label')\n",
    "Y_R2L = R2L_df['label']\n",
    "X_U2R = U2R_df.drop(columns='label')\n",
    "Y_U2R = U2R_df['label']\n",
    "\n",
    "# test set\n",
    "X_DoS_test = DoS_df_test.drop(columns='label')\n",
    "Y_DoS_test = DoS_df_test['label']\n",
    "X_Probe_test = Probe_df_test.drop(columns='label')\n",
    "Y_Probe_test = Probe_df_test['label']\n",
    "X_R2L_test = R2L_df_test.drop(columns='label')\n",
    "Y_R2L_test = R2L_df_test['label']\n",
    "X_U2R_test = U2R_df_test.drop(columns='label')\n",
    "Y_U2R_test = U2R_df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames=list(X_DoS)\n",
    "colNames_test=list(X_DoS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = preprocessing.StandardScaler().fit(X_DoS)\n",
    "X_DoS=scaler1.transform(X_DoS) \n",
    "scaler2 = preprocessing.StandardScaler().fit(X_Probe)\n",
    "X_Probe=scaler2.transform(X_Probe) \n",
    "scaler3 = preprocessing.StandardScaler().fit(X_R2L)\n",
    "X_R2L=scaler3.transform(X_R2L) \n",
    "scaler4 = preprocessing.StandardScaler().fit(X_U2R)\n",
    "X_U2R=scaler4.transform(X_U2R) \n",
    "# test data\n",
    "scaler5 = preprocessing.StandardScaler().fit(X_DoS_test)\n",
    "X_DoS_test=scaler5.transform(X_DoS_test) \n",
    "scaler6 = preprocessing.StandardScaler().fit(X_Probe_test)\n",
    "X_Probe_test=scaler6.transform(X_Probe_test) \n",
    "scaler7 = preprocessing.StandardScaler().fit(X_R2L_test)\n",
    "X_R2L_test=scaler7.transform(X_R2L_test) \n",
    "scaler8 = preprocessing.StandardScaler().fit(X_U2R_test)\n",
    "X_U2R_test=scaler8.transform(X_U2R_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DoS.std(axis=0)\n",
    "X_Probe.std(axis=0)\n",
    "X_R2L.std(axis=0)\n",
    "X_U2R.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=clf, n_features_to_select=13, step=1)\n",
    "rfe.fit(X_DoS, Y_DoS.astype(int))\n",
    "X_rfeDoS=rfe.transform(X_DoS)\n",
    "X_rfeDoS_test=rfe.transform(X_DoS_test)\n",
    "true=rfe.support_\n",
    "rfecolindex_DoS=[i for i, x in enumerate(true) if x]\n",
    "rfecolname_DoS=list(colNames[i] for i in rfecolindex_DoS)\n",
    "print(rfecolname_DoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X_Probe, Y_Probe.astype(int))\n",
    "X_rfeProbe=rfe.transform(X_Probe)\n",
    "X_rfeProbe_test=rfe.transform(X_Probe_test)\n",
    "true=rfe.support_\n",
    "rfecolindex_Probe=[i for i, x in enumerate(true) if x]\n",
    "rfecolname_Probe=list(colNames[i] for i in rfecolindex_Probe)\n",
    "print(rfecolname_Probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X_R2L, Y_R2L.astype(int))\n",
    "X_rfeR2L=rfe.transform(X_R2L)\n",
    "X_rfeR2L_test=rfe.transform(X_R2L_test)\n",
    "true=rfe.support_\n",
    "rfecolindex_R2L=[i for i, x in enumerate(true) if x]\n",
    "rfecolname_R2L=list(colNames[i] for i in rfecolindex_R2L)\n",
    "print(rfecolname_R2L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X_U2R, Y_U2R.astype(int))\n",
    "X_rfeU2R=rfe.transform(X_U2R)\n",
    "X_rfeU2R_test=rfe.transform(X_U2R_test)\n",
    "true=rfe.support_\n",
    "rfecolindex_U2R=[i for i, x in enumerate(true) if x]\n",
    "rfecolname_U2R=list(colNames[i] for i in rfecolindex_U2R)\n",
    "print(rfecolname_U2R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features selected for DoS:',rfecolname_DoS)\n",
    "print()\n",
    "print('Features selected for Probe:',rfecolname_Probe)\n",
    "print()\n",
    "print('Features selected for R2L:',rfecolname_R2L)\n",
    "print()\n",
    "print('Features selected for U2R:',rfecolname_U2R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\")\n",
    "print(X_rfeDoS.shape)\n",
    "print(X_rfeProbe.shape)\n",
    "print(X_rfeR2L.shape)\n",
    "print(X_rfeU2R.shape)\n",
    "print(\"Test:\")\n",
    "print(X_rfeDoS_test.shape)\n",
    "print(X_rfeProbe_test.shape)\n",
    "print(X_rfeR2L_test.shape)\n",
    "print(X_rfeU2R_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all features\n",
    "clf_DoS=RandomForestClassifier(random_state=0)\n",
    "clf_Probe=RandomForestClassifier(random_state=0)\n",
    "clf_R2L=RandomForestClassifier(random_state=0)\n",
    "clf_U2R=RandomForestClassifier(random_state=0)\n",
    "clf_DoS.fit(X_DoS, Y_DoS.astype(int))\n",
    "clf_Probe.fit(X_Probe, Y_Probe.astype(int))\n",
    "clf_R2L.fit(X_R2L, Y_R2L.astype(int))\n",
    "clf_U2R.fit(X_U2R, Y_U2R.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected features\n",
    "clf_rfeDoS=RandomForestClassifier(random_state=0)\n",
    "clf_rfeProbe=RandomForestClassifier(random_state=0)\n",
    "clf_rfeR2L=RandomForestClassifier(random_state=0)\n",
    "clf_rfeU2R=RandomForestClassifier(random_state=0)\n",
    "clf_rfeDoS.fit(X_rfeDoS, Y_DoS.astype(int))\n",
    "clf_rfeProbe.fit(X_rfeProbe, Y_Probe.astype(int))\n",
    "clf_rfeR2L.fit(X_rfeR2L, Y_R2L.astype(int))\n",
    "clf_rfeU2R.fit(X_rfeU2R, Y_U2R.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the classifier we trained to the test data (which it has never seen before)(All features)\n",
    "Y_pred_Dos = clf_DoS.predict(X_DoS_test)\n",
    "Y_pred_Probe = clf_Probe.predict(X_Probe_test)\n",
    "Y_pred_R2L = clf_R2L.predict(X_R2L_test)\n",
    "Y_pred_U2R = clf_U2R.predict(X_U2R_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the classifier we trained to the test data (which it has never seen before)(selected features)\n",
    "Y_pred_rfeDos = clf_rfeDoS.predict(X_rfeDoS_test)\n",
    "Y_pred_rfeProbe = clf_rfeProbe.predict(X_rfeProbe_test)\n",
    "Y_pred_rfeR2L = clf_rfeR2L.predict(X_rfeR2L_test)\n",
    "Y_pred_rfeU2R = clf_rfeU2R.predict(X_rfeU2R_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(All features)\")\n",
    "pd.crosstab(Y_DoS_test, Y_pred_Dos, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Selected features)\")\n",
    "pd.crosstab(Y_DoS_test, Y_pred_rfeDos, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(All features)\")\n",
    "pd.crosstab(Y_Probe_test, Y_pred_Probe, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Selected features)\")\n",
    "pd.crosstab(Y_Probe_test, Y_pred_rfeProbe, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(All features)\")\n",
    "pd.crosstab(Y_R2L_test, Y_pred_R2L, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Selected features)\")\n",
    "pd.crosstab(Y_R2L_test, Y_pred_rfeR2L, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(All features)\")\n",
    "pd.crosstab(Y_U2R_test, Y_pred_U2R, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Selected features)\")\n",
    "pd.crosstab(Y_U2R_test, Y_pred_rfeU2R, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(All Features)\")\n",
    "accuracy = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='precision')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='recall')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='f1')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Selected Features)\")\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_rfeDoS_test, Y_DoS_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeDoS, X_rfeDoS_test, Y_DoS_test, cv=10, scoring='precision')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeDoS, X_rfeDoS_test, Y_DoS_test, cv=10, scoring='recall')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeDoS, X_rfeDoS_test, Y_DoS_test, cv=10, scoring='f1')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(All Features)\")\n",
    "accuracy = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Selected Features)\")\n",
    "accuracy = cross_val_score(clf_rfeProbe, X_rfeProbe_test, Y_Probe_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeProbe, X_rfeProbe_test, Y_Probe_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeProbe, X_rfeProbe_test, Y_Probe_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeProbe, X_rfeProbe_test, Y_Probe_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(All Features)\")\n",
    "accuracy = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Selected Features)\")\n",
    "accuracy = cross_val_score(clf_rfeR2L, X_rfeR2L_test, Y_R2L_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeR2L, X_rfeR2L_test, Y_R2L_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeR2L, X_rfeR2L_test, Y_R2L_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeR2L, X_rfeR2L_test, Y_R2L_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(All Features)\")\n",
    "accuracy = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Selected Features)\")\n",
    "accuracy = cross_val_score(clf_rfeU2R, X_rfeU2R_test, Y_U2R_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeU2R, X_rfeU2R_test, Y_U2R_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeU2R, X_rfeU2R_test, Y_U2R_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeU2R, X_rfeU2R_test, Y_U2R_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values=shap.TreeExplainer(clf_rfeDoS).shap_values(X_rfeDoS_test)\n",
    "shap_interaction_values=shap.TreeExplainer(clf_rfeDoS).shap_interaction_values(X_rfeDoS_test)\n",
    "res = shap_values[:,:,0]\n",
    "X_rfeDoS_test_df = pd.DataFrame(X_rfeDoS_test, columns=rfecolname_DoS)\n",
    "shap.summary_plot(res, X_rfeDoS_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(res, X_rfeDoS_test_df , plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values=shap.TreeExplainer(clf_rfeProbe).shap_values(X_rfeProbe_test)\n",
    "shap_interaction_values=shap.TreeExplainer(clf_rfeProbe).shap_interaction_values(X_rfeProbe_test)\n",
    "res = shap_values[:,:,0]\n",
    "X_rfeProbe_test_df = pd.DataFrame(X_rfeProbe_test, columns=rfecolname_Probe)\n",
    "shap.summary_plot(res, X_rfeProbe_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(res, X_rfeProbe_test_df,plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values=shap.TreeExplainer(clf_rfeU2R).shap_values(X_rfeU2R_test)\n",
    "shap_interaction_values=shap.TreeExplainer(clf_rfeU2R).shap_interaction_values(X_rfeU2R_test)\n",
    "res = shap_values[:,:,0]\n",
    "X_rfeU2R_test_df = pd.DataFrame(X_rfeU2R_test, columns=rfecolname_U2R)\n",
    "shap.summary_plot(res, X_rfeU2R_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(res, X_rfeU2R_test_df, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values=shap.TreeExplainer(clf_rfeR2L).shap_values(X_rfeR2L_test)\n",
    "shap_interaction_values=shap.TreeExplainer(clf_rfeR2L).shap_interaction_values(X_rfeR2L_test)\n",
    "res = shap_values[:,:,0]\n",
    "X_rfeR2L_test_df = pd.DataFrame(X_rfeR2L_test, columns=rfecolname_R2L)\n",
    "shap.summary_plot(res, X_rfeR2L_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(res, X_rfeR2L_test_df, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
